{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "%matplotlib inline\n",
    "import random\n",
    "import time\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#return a list of image filenames\n",
    "def get_img_name(f_path = '../data/images/', f_type = 'jpg'):\n",
    "    file_path, file_type = f_path, f_type\n",
    "    img_names = glob('{0}*.{1}'.format(file_path, file_type))\n",
    "    return img_names\n",
    "\n",
    "#testing the availability of image at the same time\n",
    "def label_img(img_name_list):\n",
    "    count = 0\n",
    "    img_set, labels = [], []\n",
    "    for i in img_name_list:\n",
    "        #Test whether the image is corrupted\n",
    "        #This part can be skipped to speed up the process\n",
    "        #test_read = cv2.imread(i)\n",
    "        #if test_read is None:\n",
    "        #    count += 1\n",
    "        #    continue\n",
    "        img_set.append(i)\n",
    "        #Chicken => 0 Dog => 1\n",
    "        labels.append(0 if i.split('/')[-1][0].lower()=='c' else 1)\n",
    "    #print '{} image(s) is(are) corrupted!'.format(count)\n",
    "    return np.array(img_set), np.array(labels)    \n",
    "\n",
    "img_names = get_img_name()\n",
    "image_list, labels = label_img(img_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SIFT Feature extraction\n",
    "#------Note on Feature extraction ---------\n",
    "#input format: list of image names ; output format: list of 2000 ndarrays with 128 columns(in both SIFT and SURF)\n",
    "def sift_extract(image_list, n = 50, conThre = 0.04, edgeThre = 10):\n",
    "    raw_features = []\n",
    "    for i in image_list:   \n",
    "        img = cv2.imread(i,0) #read in as grey scale\n",
    "        #bunch of parameter to be tuned\n",
    "        sift = cv2.xfeatures2d.SIFT_create(nfeatures = n,\n",
    "                                          contrastThreshold = conThre,\n",
    "                                          edgeThreshold = edgeThre)\n",
    "        kps, descs = sift.detectAndCompute(img, None)\n",
    "        raw_features.append(descs)\n",
    "    return raw_features\n",
    "\n",
    "#%time raw_features = sift_extract(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.00088771e-03,   1.35109061e-03,  -1.70919720e-05, ...,\n",
       "          5.38815875e-05,   3.67899629e-04,   3.67899629e-04],\n",
       "       [ -2.46754771e-05,   2.11617371e-04,   1.97584828e-04, ...,\n",
       "          1.46637220e-04,   3.89241977e-05,   4.57188034e-05],\n",
       "       [ -1.30757122e-04,   1.85932746e-04,   1.41760494e-04, ...,\n",
       "          1.30489774e-04,   5.37563210e-05,   8.22278671e-05],\n",
       "       ..., \n",
       "       [ -2.01411531e-04,   3.88131797e-04,  -2.24507577e-03, ...,\n",
       "          2.19792663e-03,  -5.52453520e-03,   5.52453520e-03],\n",
       "       [ -3.00993452e-05,   6.38977290e-05,   1.27058185e-03, ...,\n",
       "          1.14555005e-05,  -1.44831138e-03,   1.56030280e-03],\n",
       "       [ -7.97574583e-04,   9.71933012e-04,  -1.73443416e-03, ...,\n",
       "          2.45461095e-04,   9.76310112e-04,   1.52022624e-03]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SUFT Feature extraction\n",
    "def surf_extract(image_list, hessian = 4000):\n",
    "    raw_features = []\n",
    "    for i in image_list:\n",
    "        img = cv2.imread(i, 0)\n",
    "        surf = cv2.xfeatures2d.SURF_create(hessianThreshold = hessian, extended = True)\n",
    "        kp, descs = surf.detectAndCompute(img, None)\n",
    "        \n",
    "        #for specific image if number of keypoints is 0 (hessian value too big),\n",
    "        #gradually reduce hessian value by 500 untill there're keypoints detected from the image\n",
    "        temp_hessian = hessian\n",
    "        while descs is None:\n",
    "            temp_hessian -= 500\n",
    "            surf = cv2.xfeatures2d.SURF_create(hessianThreshold = temp_hessian, extended = True)\n",
    "            kp,descs = surf.detectAndCompute(img, None)\n",
    "    \n",
    "        \n",
    "        raw_features.append(descs)\n",
    "    return raw_features\n",
    "\n",
    "#%time raw_features = surf_extract(image_list, hessian = 5000)\n",
    "raw_features[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combine features into a single ndarray and return a vector of group label   \n",
    "#group label used for mapping the result cluster center into corresponding image\n",
    "def combine_feature(raw_feature_list):\n",
    "    feature_array = np.vstack(raw_feature_list)\n",
    "    group_number = [x.shape[0] for x in raw_feature_list]\n",
    "    group_label = np.repeat(range(len(raw_feature_list)), group_number)\n",
    "    return feature_array, group_label\n",
    "\n",
    "#---------Note on clustering---------\n",
    "#input format: list of ndarray, output format single ndarray with dimension(n_image, n_cluster)\n",
    "def kmeans_cluster(raw_feature_list, sub_sample = False, sub_perc = 0.5, n_cluster = 1000, max_Iter = 50):\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    features = np.zeros((len(raw_feature_list), n_cluster))\n",
    "    feature_array, group_label = combine_feature(raw_feature_list)\n",
    "    #dtype check, same RAM\n",
    "    if not feature_array.dtype == 'float32':\n",
    "        feature_array.dtype == 'float32'\n",
    "    \n",
    "    #if number of features is larger than sub_perc*feature number and sub_sample is on\n",
    "    #randomly subsample features\n",
    "    if sub_sample and (sub_perc * feature_array.shape[0])>n_cluster:\n",
    "        n_row  = feature_array.shape[0]\n",
    "        sub_sample_index = random.sample(range(n_row), int(sub_perc*n_row))\n",
    "        feature_array = feature_array[sub_sample_index,:]\n",
    "        group_label = group_label[sub_sample_index]\n",
    "\n",
    "    #In the case when cluster number is larger than data dimention\n",
    "    #change cluster number to half of the feature \n",
    "    if feature_array.shape[0] < n_cluster:\n",
    "        n_cluster = feature_array.shape[0]/2\n",
    "        \n",
    "    km = KMeans(n_clusters = n_cluster, max_iter = max_Iter).fit(feature_array)\n",
    "    cluster_label = km.labels_\n",
    "    #assign every feature (row) to its corresponding group\n",
    "    for i in range(feature_array.shape[0]):\n",
    "        features[group_label[i],cluster_label[i]] += 1\n",
    "    #convert to probability\n",
    "    features = np.apply_along_axis(lambda x: x/sum(x), 1, features)\n",
    "    return features\n",
    "\n",
    "#%time result = kmeans_cluster(raw_features, sub_sample=True, n_cluster=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mini_kmeans(raw_feature_list, n_cluster = 1000, sample_size = 100, Iter = 50):\n",
    "    #Randomly sample between for each iteration\n",
    "    from sklearn.cluster import MiniBatchKMeans\n",
    "    \n",
    "    features = np.zeros((len(raw_feature_list), n_cluster))\n",
    "    feature_array, group_label = combine_feature(raw_feature_list)\n",
    "    #dtype check, save RAM\n",
    "    if not feature_array.dtype == 'float32':\n",
    "        feature_array.dtype == 'float32'\n",
    "        \n",
    "    #In the case when cluster number is larger than data dimention\n",
    "    #change cluster number to half of the feature \n",
    "    if feature_array.shape[0] < n_cluster:\n",
    "        n_cluster = feature_array.shape[0]/2\n",
    "        \n",
    "    mini_kmeans = MiniBatchKMeans(n_clusters = n_cluster, batch_size = sample_size, \n",
    "                                  max_iter = Iter, init_size = 12000).fit(feature_array)\n",
    "    cluster_label = mini_kmeans.labels_\n",
    "    #assign feature to group\n",
    "    for i in range(feature_array.shape[0]):\n",
    "        features[group_label[i],cluster_label[i]] += 1\n",
    "    #convert to probability\n",
    "    features = np.apply_along_axis(lambda x: x/sum(x), 1, features)\n",
    "    return features\n",
    "\n",
    "#%time result = mini_kmeans(raw_features, n_cluster = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#set background color to black\n",
    "def get_color_dist(image_names):\n",
    "    color_dist_list = []\n",
    "    for i in range(len(image_names)):    \n",
    "        img = cv2.imread(image_names[i], 1)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            #get background mask\n",
    "        _, mask = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "        B_dist = cv2.calcHist([img], [0], mask, [256], [0,256]).ravel()\n",
    "        G_dist = cv2.calcHist([img], [0], mask, [256], [0,256]).ravel()\n",
    "        R_dist = cv2.calcHist([img], [0], mask, [256], [0,256]).ravel()\n",
    "        \n",
    "        #standardized to exclude the influence of image size\n",
    "        B_dist = B_dist/B_dist.sum()\n",
    "        G_dist = G_dist/G_dist.sum()\n",
    "        R_dist = R_dist/R_dist.sum()\n",
    "        \n",
    "        color_dist_list.append(np.hstack([B_dist, G_dist, R_dist]))\n",
    "    return np.vstack(color_dist_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#return train data\n",
    "def preprocess(f_path = '../data/images/', f_type = 'jpg',\n",
    "               feature_method = 'SIFT', n_keypoint = 50, hessian_thre = 3000,\n",
    "               cluster_method = 'KMeans',sub = False, perc = 0.5, clusters = 1000, Iter = 50, sample_size = 100):\n",
    "    images, labels = label_img(get_img_name(f_path = f_path, f_type = f_type))\n",
    "    if feature_method == 'SIFT':\n",
    "        raw_features = sift_extract(images, n = n_keypoint)\n",
    "    elif feature_method == 'SURF':\n",
    "        raw_features = surf_extract(images, hessian= hessian_thre)\n",
    "    if cluster_method == 'KMeans':\n",
    "        features = kmeans_cluster(raw_features,sub_sample = sub , sub_perc = perc, n_cluster = clusters, max_Iter = Iter)\n",
    "    elif cluster_method == 'MiniKMeans':\n",
    "        features =  mini_kmeans(raw_features, n_cluster=clusters, sample_size = sample_size, Iter = Iter)\n",
    "    \n",
    "    color_features = get_color_dist(images)\n",
    "    features = np.hstack([features, color_features])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 37s, sys: 2min 9s, total: 8min 47s\n",
      "Wall time: 7min 26s\n",
      "0.800833333333\n"
     ]
    }
   ],
   "source": [
    "#TEST USING SVM\n",
    "\n",
    "%time x, y = preprocess(clusters = 5500, feature_method='SIFT', hessian_thre = 2600, Iter = 50, n_keypoint= 60, cluster_method = 'MiniKMeans')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ShuffleSplit, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "#base_sift = pd.read_csv('/Users/Max/GitHub/Fall2016-proj3-grp1/data/sift_features.csv')\n",
    "#base_sift = base_sift.transpose()\n",
    "#x = base_sift.values\n",
    "    \n",
    "rs = ShuffleSplit(n_splits = 4, test_size = 0.3, random_state=0)\n",
    "\n",
    "score = []\n",
    "for train_index, test_index in rs.split(x):\n",
    "    train_x, train_y = x[train_index,:], y[train_index]\n",
    "    test_x, test_y = x[test_index,:], y[test_index]\n",
    "    clf = SVC(gamma = 1, C = 100)\n",
    "    clf.fit(train_x, train_y)\n",
    "    score.append(clf.score(test_x, test_y))\n",
    "print np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run preprocess, feed data to R\n",
    "#if __name__ == '__main__':\n",
    "#    features, labels = preprocess(f_path= '/Users/Max/Downloads/test/')\n",
    "np.savetxt('SIFT+COLOR.csv',x, delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
